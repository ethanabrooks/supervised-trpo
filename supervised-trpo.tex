\documentclass{article}
\usepackage{amsfonts, amsmath, amssymb, todonotes, titlesec, graphicx}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} 
\usepackage{apxproof}
\usepackage{hyperref}
\usepackage{tikz}


\newcommand{\amin}{a_{\min}}
\newcommand{\amax}{a_{\max}}
\newcommand{\prob}[2] {P_{#1}\left(#2\right)}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\dequal}{\overset{d}{=}}
\newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{A}}
\newcommand{\transitions}[1][ss']{\mathcal{P}_{#1}^a}
\newcommand{\realscalars}{\mathbb{R}}
\newcommand{\realvectors}[1]{\mathbb{R}^{#1}}
\newcommand{\grad}[1] {\nabla_{#1}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\del}{\nabla}
\newcommand{\real}{\mathbb{R}}
\newcommand{\fisher}{\mathcal{F}}
\newcommand{\Dt}{\Delta \theta}
\newcommand{\dt}{\nabla_{\theta}}
\newcommand{\ALPHA}{\boldsymbol{\alpha}}
\newcommand{\E}[2] {E_{#1}\left[#2\right]}
\newcommand{\tasks}{\mathcal{T}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\abs}[1]{\left\lVert#1\right\rVert}
\newcommand{\Dkl}{\mathcal{D}\infdivx}
\newtheorem{theorem}{Theorem}
\newtheoremrep{theorem}{Theorem}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\unif}{unif}
\bibliographystyle{unsrt}

\begin{document}
Trust Region Policy Optimization (TRPO) \cite{trpo} is an on-policy algorithm that
guarantees monotonic policy improvement. 
TRPO optimizes the following loss:
\begin{gather}
	\label{opt}
	\max_\theta\E{s\sim\rho_\pi, a\sim\pi(\cdot|s)}{\frac{\pi_\theta(a|s)}{\pi(a|s)}Q^\pi(s, a)}
	\intertext{subject to }
	\label{constraint}
	\E{s\sim\rho_\pi}{\Dkl{\pi(\cdot|s)}{\pi_\theta(\cdot|s)}} < \delta
\end{gather}
where $\pi$ is the old policy, $\pi_\theta$ is the updated policy, and
$\rho_\pi$ is the (unnormalized) discounted visitation frequencies under policy
$\pi$.
Solving this constrained optimization problem entails inverting the
Fisher-Information matrix corresponding to the KL Divergence between the old and
new policy.
This matrix inversion hurts performance, and complicates
implementation. 

Proximal Policy Optimization \cite{ppo} proposes a variation on TRPO that avoids
matrix inversion with the following loss function:
\begin{align}
  \E{s\sim\rho_\pi,
  a\sim\pi(\cdot|s)}{CLIP\left(\frac{\pi_\theta(a|s)}{\pi(a|s)}, 1 - \epsilon, 1
+ \epsilon\right)Q^\pi(s, a)}
\intertext{with}
CLIP(x, a, b) = \begin{cases}
  x & \text{if $a < x < b$} \\
  a & \text{if $x \le a$} \\
  b & \text{if $x \ge b$} \\
\end{cases}
\end{align}
The $CLIP$ constraint approximately enforces the TRPO constraint (expression
\ref{constraint}).
In most implementations, PPO performs multiple small gradient steps on each
batch of experience. Per the loss function, each step is collinear with the
unconstrained gradient until $\abs{1 - \pi_\theta(a|s)/\pi(a|s)} >
\epsilon$, at which point the gradient becomes 0.

PPO demonstrates strong empirical performance and avoids many of the
implementation and performance pitfalls of TRPO. However, unlike TRPO, the PPO
gradient step is suboptimal:
\begin{enumerate}
  \item The policy update can violate the TRPO
constraint for one step (since the PPO loss function is only evaluated at the
current parameters). In principle, this forces PPO to use a smaller
learning rate than it would have to otherwise and perform many time-intensive
gradient calculations.
\item Once the $CLIP$ constraint is violated, the gradient goes to zero, despite
  the fact that in general, it may be possible to continue improving the policy
  without increasing KL Divergence (by stepping the parameters along the level
  curve of the KL Divergence). This results in idle GPU units when
  a batch of experiences includes a mix of clipped and unclipped ratios.
\item The direction of the parameter update is suboptimal. In general, the
  vector to
  the optimal parameters within the feasible set will not be collinear with the
  unconstrained gradient vector (the gradient of expression \ref{opt}). On the contrary, it will be the vector that
  maximizes the \textit{projection} along the unconstrained gradient. The
  optimal gradient is only collinear with the unconstrained gradient when the
  Fisher Information is collinear with the identity matrix, which in general is
  not true.
\end{enumerate}

We propose an alternate algorithm that combines the advantages of TRPO --
provably monotonic policy improvement, with the advantages of PPO -- simple
implentation and fast performance.
The key idea is that expression \ref{opt} is convex in $\pi_\theta$ and
therefore one can analytically compute the optimal policy. With this in hand,
the actual policy can be trained to match the optimal policy using supervised
learning.
\begin{theoremrep}
  The monotonic improvement guarantee in the TRPO paper (section 3) still applies if the KL
  Divergence in expression \ref{constraint} is reversed.
  \label{reverse}
\end{theoremrep}
\begin{proof}
  The assertion about monotonic improvement in the original paper depends on the
  fact that 
  KL Divergence is an upper bound on ``total variation divergence'':
  \begin{align}
    \mathcal{D}_{TV}\infdivx{p}{q} &= \frac{1}{2}\sum_i\abs{p_i - q_i}
    \intertext{Clearly}
    \mathcal{D}_{TV}\infdivx{p}{q} &= \mathcal{D}_{TV}\infdivx{q}{p}
    \intertext{Therefore, the assertion in the paper, that}
    \mathcal{D}_{TV}\infdivx{p}{q}^2 &< \Dkl{p}{q}
    \intertext{implies that}
    \mathcal{D}_{TV}\infdivx{p}{q}^2 &< \Dkl{q}{p}
\end{align}
Therefore the reversed KL Divergence is still an upper bound and the monotonic
improvement guarantee still holds. 
\end{proof}
\begin{theoremrep}
	Let $\pi$ indicate the current (or old) policy and let $\pi_\theta$ indicate
	the updated policy. Then the optimal policy is
  \begin{align}
    \pi_\theta(a|s) 
                      &= \text{softmax}\left(\log\pi(\cdot|s) + \alpha Q^\pi(s,
                      \cdot)\right)
  \end{align}
  where $\alpha$ is the largest value that does not violate constraint
  \ref{constraint}
\end{theoremrep}
\begin{proof}
First we not that equation \ref{opt} is convex in $\pi_\theta$ under the
assumption that all rewards are positive. Before applying the Karush-Kuhn-Tucker Conditions, we reverse the KL Divergence in
accordance with theorem \ref{reverse}.


For each $(s, a) \in
	\states \times \actions$:
\begin{align}
	 & \grad{\pi_\theta(a|s)}\E{s'\sim\rho_\pi,
		a'\sim\pi(\cdot|s')}{\frac{\pi_\theta(a'|s')}{\pi(a'|s')}Q^\pi(s', a')}
	\\
	\begin{split}
		&\quad =                 \grad{\pi_\theta(a|s)}
		\mu\left(\E{s'\sim\rho_\pi}{\Dkl{\pi_\theta(\cdot|s')}{\pi(\cdot|s')}} - \delta\right)
		\\
    &\quad -
    \grad{\pi_\theta(a|s)}\sum_{s'\in\states}\sum_{a'\in\actions}\mu_{s',
    a'}\pi_\theta(a'|s')
    \\
		&\quad +                  \grad{\pi_\theta(a|s)}\sum_{s'\in\states}\lambda_{s'}\left(1-
		\sum_{a'\in\actions}\pi_\theta(a'|s')\right)
	\end{split}
\end{align}
where the last two terms relate to the simplex constraints.
Then the left side is $\rho_\pi(s)Q^\pi(s, a)$. The first term on the
right is
\begin{align}
	 & \mu\rho_\pi(s)\grad{\pi_\theta(a| s)}\left(\sum_{a' \in
			\actions}\pi_\theta(a'|s)\left[\log\pi_\theta(a'|s) - \log\pi(a'|s) \right] - \delta\right)
	\\
   & \quad =                 \mu\rho_\pi(s)\left[\log \pi_\theta(a|s) + 1 - \log
   \pi(a|s)\right]
\end{align}
The last two terms on the right are $-\mu_{s,a} + \lambda_s$. Thus \begin{align}
	\label{def1}
    \pi_\theta(a|s) &= \pi(a|s)\beta\gamma^{Q^\pi(s, a)}
    \intertext{where}
    \beta &\defeq \exp\left(\frac{\mu_{s, a} - \lambda_s}{\mu\rho_\pi(s)} -1\right)
    \\
    \gamma &\defeq \exp(1/\mu)
  \intertext{Because $\sum_{a\in\actions} \pi_\theta(a|s) = 1$,}
    \beta &= \frac{1}{\E{a\sim\pi}{\gamma^{Q^\pi(s, a)}}}
  \end{align}
  Moreover, one may easily confirm by inspection or by taking the derivative
  that expression \ref{opt} increases with $\gamma$ when $\pi_\theta$ is defined
  according to \ref{def1}. Therefore the solution to the optimization problem
  will use the largest value of $\gamma$ that does not violate the contraint
  \ref{constraint}. Note that if $\gamma = 0$, the KL Divergence is zero and as
  $\gamma \to \infty$, the KL Divergence approaches $-\log \pi(\argmax_a
  Q^\pi(s, a)|s)$. Setting $\alpha = \log \gamma$ achieves the promised result.

\end{proof}
\bibliography{bib}
\end{document}

